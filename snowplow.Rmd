---
title: "City of Syracuse Snowplow Analysis"
author: "James Mason"
output: html_document
rows.print: 10
---

With an average yearly snowfall of 126.3 inches, Syracuse, New York is the [snowiest city in the U.S.](https://weather.com/storms/winter/news/20-snowiest-large-cities-america-20140130#/20). In order to keep roads safe, snow and ice removal is an important industry in the city. Today we will be exploring a snowplow data from January, 3rd, 2018. The dataset describes the location of several different snow plows at different times, as well as what they were doing.

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Import Data

First we want to import our dataset. It can be downloaded [here](http://data.syrgov.net/datasets/0fe934c49ad84223a695aa791a54c1c1_0).

A basic description of the variables in the dataset can be found [here](http://github.com/CityofSyracuse/OpenDataDictionaries/blob/master/snowplow_DataDictionary.csv).

We will use read_csv from readr to import the dataset, so we need to load it. Since we will be using most libraries in tidyverse, we will load the tidyverse library.

```{r load_tidyverse}
library(tidyverse)
```

```{r load_dataset}
plow_df <- read_csv("data/Snowplow_Data_January_3_2018.csv")

plow_df
```

A brief look at the dataset shows that we have more columns than the documentation describes. X is identical to the longitude variable, and Y is the same as the latitude. These columns can be safely removed.

All of the trucks in our dataset are snowplows, so they are all under the category "SNOW & ICE". It is therefore safe to remove repair_type.

X3 is a row ID number. It appears to come from a different table. We also have OBJECTID, which is the row number for each entity in this specific table. We therefore don't need X3 as we can use OBJECTID if we need unique identifiers.

In order to remove the variables we don't need, we will use select() from dplyr. 

```{r unselect_cols}
simple_plow_df <- plow_df %>%
  select(-X, -Y, -X3, -repair_type)

head(simple_plow_df)
```

## Tidying the Dataset

Most of the columns in our dataset are fairly straightforward. truck_name gives the truck ID number, date_fixed gives the date, etc. The column that is untidy is activity_type. 

Activity type gives the specific tasks that the truck did in the last minute. However, every entity doesn't have an activity_type associated with it because that activity continues to occur until otherwise specified. For example, after "Plow Down", the truck continues to have its plow down until "Plow Up".

We need to make it so that each entity has a variable that describes each of the possible actions. We will need variables that describe the status of the ignition, the plow, the spreader, the auxiliary motor, and whether it is moving or not.

### Prasing activity_type
First I'm going to parse activity_type, and split it up into new variables using mutate and case_when. If it is the beginning of an action, I will mark it with a 2. If it is the end, I will mark it with a 1. If it doesn't contain the action it will be marked with a 0.

```{r split_activity_type}
split_plow_df <- simple_plow_df %>%
  mutate(moving = case_when(
    grepl("Start Moving", activity_type) ~ 2,
    grepl("Stop Moving", activity_type) ~ 1,
    TRUE ~ 0
  )) %>%
  
  mutate(ignition = case_when(
    grepl("Ignition On", activity_type) ~ 2,
    grepl("Ignition Off", activity_type) ~ 1,
    TRUE ~ 0
  )) %>%
  
  mutate(plow = case_when(
    grepl("Plow Down", activity_type) ~ 2,
    grepl("Plow Up", activity_type) ~ 1,
    TRUE ~ 0
  )) %>%
  
  mutate(spreader = case_when(
    grepl("Spreader On", activity_type) ~ 2,
    grepl("Spreader Off", activity_type) ~ 1,
    TRUE ~ 0
  )) %>%
  
  mutate(aux_motor = case_when(
    grepl("Aux Motor On", activity_type) ~ 2,
    grepl("Aux Motor off", activity_type) ~ 1,
    TRUE ~ 0
  ))
```


### Assigning State Variables

Now that we've parsed activity_type, we need to go through and set all of the variables between the bounds to be either on or off. 

In other words, for every single truck, when ordered by time, we need to assign $variable(t_i) = 1, \forall t_i \in (t_s,t_e)$ where $t_s$ is the time when the variable becomes TRUE and $t_e$ is the time when the variable becomes FALSE.

One issue that we will have to account for is that some of the snow plows were working at time 00:00:00 which is the start of the dataset. For these trucks we may see a command that doesn't appear applicable. For example, we may see "Plow Up" when we haven't already seen a "Plow Down". If this occurs, we can reasonably assume that the feature was in the on position at the start of the dataset.

```{r assign_inner_vars}
#Make an empty tible. We will append tempary tibbles to it
completed_df <- filter(split_plow_df, truck_name == "0")

for (truck in levels(factor(split_plow_df$truck_name))){
  #select the rows only relevent to a specific truck
  temp <- filter(split_plow_df, truck_name == truck)
  
  #sort the rows by time (OBJECTID is sorted)
  temp <- temp[order(temp$date_fixed),]
  
  for (var in c("moving", "ignition", "plow", "spreader", "aux_motor")){
    unseen <- TRUE
    toggle <- 0
    
    for (i in seq(1, nrow(temp))){
      if (temp[[i,var]] == 1){
        #checks if the earlier moves should be on
        if (unseen & i > 1){
          for (j in seq(1, i - 1)){
            temp[[j,var]] <- 1
          }
        }
        toggle <- 0
      }else if (temp[[i,var]] == 2){
        toggle <- 1
        unseen <- FALSE
      }
      
      temp[[i,var]] <- toggle
    }
  }
    
  #use rbind to combine the table back together
  completed_df <- rbind(completed_df, temp) 
}

head(completed_df)
```





## Data Exploration

Now that we have tidied the data, we can begin to explore it. We want to look for interesting facts and trends.

### Basic Truck Analysis

One piece of interest is trying to better understand how these teams of snow plows work. For example, are there some trucks that only plow snow while other trucks only spread salt or sand? What streets tend to be plowed more often?


First let's find out how many trucks we have in the entire dataset. We can do this by changing truck_name to be a factor, and then calling the levels() function to get a vector of all trucks.
```{r num_trucks}
trucks <- levels(factor(completed_df$truck_name))

length(trucks)
```

So 32 trucks are represented in the dataset, so they were at least turned on. However, by combing through the dataset you notice that some of the trucks never even leave the lot. Let's filter the data to only show trucks that moved.
```{r num_used_trucks}
used_trucks <- levels(factor(filter(completed_df, moving == 1)$truck_name))

length(used_trucks)
```

Similarly, let's find out how many trucks plowed snow. This time, we first filter our dataset to only consider trucks that had their plow lowered.
```{r num_plow_trucks}
plow_trucks <- levels(factor(filter(completed_df, plow == 1)$truck_name))

length(plow_trucks)
```

Similarly, the number of trucks that used the spreader, likely to spread salt or sand. 
```{r num_spread_trucks}
spread_trucks <- levels(factor(filter(completed_df, spreader == 1)$truck_name))

length(spread_trucks)
```

Now the number of trucks that used both the spreader and plow.
```{r num_plow_spread_trucks}
plow_spread_trucks <- levels(factor(filter(completed_df, spreader == 1, plow == 1)$truck_name))

length(plow_spread_trucks)
```
```{r compare_sets}
spread_trucks == plow_spread_trucks
```

We can see that all of the trucks that used a spreader also plowed. In fact they did both at the same time at least one point in their run.

While 20 trucks moved, only 13 actually plowed. What happened to the remaining 7? We can figure out what trucks moved but didn't plow snow by using setdiff.

```{r missing_trucks}
mia_trucks <- setdiff(used_trucks, plow_trucks)

mia_trucks
```


One variable that we haven't looked at yet is aux_motor. I haven't discussed this yet because it is ambiguous as to what it means. Turning on the auxiliary motor could be operating a plow or a spreader 

```{r non_label_trucks}
aux_motor_trucks <- levels(factor(filter(completed_df, aux_motor == 1)$truck_name))

non_labeled_trucks <- setdiff(mia_trucks, aux_motor_trucks)

non_labeled_trucks
```

Of the trucks that didn't use a plow, spreader, or auxiliary motor, truck 257 is of particular interest. It drove from the DPW at 1200 Canal Street to a gas station at 3100 Erie Boulevard, and then back to 1200 Canal Street. We can therefore infer that the City of Syracuse DPW does not own their own gas station.

The other trucks in non_labeled_trucks appear to be driving around. They may be plowing snow, but lack the sensors to report exactly what they are doing. 

Since we don't know what they are doing at different times, we will focus our analysis on trucks that reported using their plow or spreader. This also comes with an additional benefit. The trucks that reported using their plow or spreader describe their position and actions each minute. Some of the other trucks also report every time they do a new action. Since the reports for the trucks in plow_trucks come at regular intervals, we can do comparisons with them much more easily.

### Make Filtered Dataframes
In order to create a new dataframe that is filtered on certain kinds of trucks, we can use the %in% operator. 

Let's make a dataframe for trucks that are plowing.
```{r make_active_dataframes}
active_plow_df <- completed_df[completed_df$truck_name %in% plow_trucks & 
                              completed_df$plow == 1,]
```

Now we will do the same thing but for the trucks that don't have as accurate readers.
```{r mia_trucks_df}
mia_trucks_df <- completed_df[completed_df$truck_name %in% mia_trucks,]
```

## What areas are cleared the most?

One question of interest is what streets of the city are plowed most often. In order to do this we want to count the number of times that we visit each street while plowing.

Unfortunately, the street addresses given to us aren't perfect for the job. Firstly, we need to remove the house number that comes with the address so that we get a view of the entire street. Similarly, we need to remove additional information when the address contains "&" and "TO". These are used when the truck is on the intersection of two streets, or when they need to break up a street due to lack of house numbers. Some of the trucks also include the area code or "SYRACUSE", which also needs to be removed.

We will remove these using [regex and str_remove](https://cran.r-project.org/web/packages/stringr/vignettes/stringr.html). We will also trim any remaining white space off the ends.

```{r most_plowed_streets}
streets_df <- active_plow_df %>%
  mutate(street = str_trim(
                  str_remove(
                  str_remove(
                  str_remove(
                  str_remove(
                  str_remove(address, "\\d+.(\\d+)?"), "(&.+)|(TO .*)"), "\\d{5}"),
                            "SYRACUSE"), "\\*+")))

streets_count_df <- streets_df %>%
  count(street)
  
streets_count_df[order(streets_count_df$n, decreasing = TRUE),]

```


These results are a little strange. Canal Street is where the DPW stores their trucks, so it makes sense that the trucks are there very often. However, this data suggests that they are often plowing this area. This could be that drivers are leaving their plows down when sitting, or it could be a flaw in our data.

The high number of NA streets may be due to the slight inaccuracy in the GPS systems. Additionally, they plows may sometimes be on unmarked roads. 

It is surprising that Simon Dr. has the second most visits, considering it is quite small. Every other frequently plowed street is a major street, so why is Simon Dr. on here?

```{r simon_drive}
streets_df %>%
  filter(street == "SIMON DR") %>%
  count(truck_name)

```

It appears that truck 284 spent a lot of time sitting at Simon Drive. When looking at the data for truck 284, we can notice that it spent almost all of its time sitting at Canal Street and Simon Drive. Since this is skewing our data, further analysis will be done without including truck 284.

```{r remove_284_anal}
active_df <- active_plow_df %>%
  filter(truck_name != 284)

tidy_streets_df <- active_df %>%
  mutate(street = str_trim(
                  str_remove(
                  str_remove(
                  str_remove(
                  str_remove(
                  str_remove(address, "\\d+.(\\d+)?"), "(&.+)|(TO .*)"), "\\d{5}"),
                            "SYRACUSE"), "\\*+")))

streets_count_df1 <- tidy_streets_df %>%
  count(street)
  
streets_count_df1[order(streets_count_df1$n, decreasing = TRUE),]

```

These results are much more reasonable. We can see that James Street and I-690 Eastbound are the streets that the trucks spent the most time on.

### Truck Location Visualization

Since we have the positional coordinates of the trucks, we can visualize the different positions of the trucks onto a map. We will use [leaflet](https://rstudio.github.io/leaflet/) for this purpose.

Since a mass of points can be overwhelming, it is also nice to [color the points](https://rstudio.github.io/leaflet/colors.html). A good attribute to color the points by is the different trucks. This will let us see the areas that the different snow plows cover. We color the points by using colorFactor and the color argument of addCircleMarkers.

We have already split the data into two different groups, the more reliable plow_trucks and the less reliable mia_trucks. We can then display these groups separately on the graph by [using multiple layers](https://rstudio.github.io/leaflet/showhide.html). By assigning the markers to different groups using the group argument, we can allow the user to control which groups to see using addLayersControl().


```{r loc_map}
library(leaflet)

factpal_plow <- colorFactor(rainbow(plow_trucks, start = 0, end = 0.7), plow_trucks)
factpal_mia <- colorFactor(rainbow(mia_trucks, start = 0.71, end = 1), mia_trucks)

plow_map <- 
  leaflet() %>%
  addTiles() %>%
  addCircleMarkers(lng = active_plow_df$longitude, 
                   lat = active_plow_df$latitude,
                   radius = 2, 
                   popup = as.character(active_plow_df$truck_name), 
                   label = as.character(active_plow_df$truck_name),
                   color = factpal_plow(active_plow_df$truck_name),
                   group = "Plows") %>%
  addCircleMarkers(lng = mia_trucks_df$longitude, 
                   lat = mia_trucks_df$latitude,
                   radius = 2, 
                   popup = as.character(mia_trucks_df$truck_name), 
                   label = as.character(mia_trucks_df$truck_name),
                   color = factpal_mia(mia_trucks_df$truck_name),
                   group = "Non-plowing Trucks") %>%
  addLayersControl(
    overlayGroups = c("Plows", "Non-plowing Trucks"),
    options = layersControlOptions(collapsed = FALSE)
  ) %>%
  setView(lat=43.05, lng=-76.15, zoom=12)

plow_map
```


# Potholes filled in 2018

Salt and winter weather is famous for damaging both cars and roads. Since we know what streets were plowed the most on January 3rd, can we identify a statistically significant relationship between the time spent plowing a street and how damaged the street is? 

Another dataset that is available from the City of Syracuse is a dataset that lists most of the potholes that have been filled in by the City so far this year. It is missing some data from January and February due to a damaged sensor. It was last updated on May 17, 2018. The dataset can be found [here](http://data.syrgov.net/datasets/potholes-filled-2018).

```{r import_potholes}
potholes_df <- read_csv("data/Potholes_filled_2018.csv")

potholes_df
```

Most importantly, the dataset gives us the address that the pothole was repaired on. Can we find a relationship between the number of potholes repaired on a street and how much time was spent plowing the street?

First we need to do the same type of street name scraping that we did with the snow plow dataset. Thankfully, the addresses seem to be in a very similar format. One notable change is that these addresses are not in all capital letters. We will address this by using toupper().


```{r pothole_streets, rows.print = 10}
pothole_streets_df <- potholes_df %>%
  mutate(street = str_trim(
                  str_remove(
                  str_remove(
                  str_remove(
                  str_remove(
                  str_remove(toupper(address), "\\d+.(\\d+)?"), "(&.+)|(TO .*)"), "\\d{5}"),
                            "SYRACUSE"), "\\*+")))

p_streets_count_df <- pothole_streets_df %>%
  count(street)
  
p_streets_count_df[order(p_streets_count_df$n, decreasing = TRUE),]

```

We will try to join the dataframes by street. First we will change the "n" variable names to be more descriptive.

We will also remove Canal Street from our dataframe. Since it is where the plows are stored, it is a bit of an outlier.

Since freq_potholes has a value of NA when there were no potholes fixed on that street, we will modify freq_potholes so that it will have value 0 instead of NA. We can do this quite easily by using [] and is.na.

```{r join_pholes_plows}
pothole_plow_df <- plyr::join(rename(streets_count_df1, plow_visits = n),
                              rename(p_streets_count_df, freq_potholes = n),
                              by = "street")

pothole_plow_df[is.na(pothole_plow_df)] <- 0

pothole_plow_df <- pothole_plow_df %>% filter(street != "CANAL ST")

head(pothole_plow_df)
```


Let's first make a plot of pothole frequency vs the number of plow visits. This scatter plot will help us get an idea of the shape of the distribution.

```{r pothole_plow_scatter}
pothole_plow_df %>%
  ggplot(mapping = aes(x= plow_visits, y= freq_potholes)) +
  geom_point() +
  labs(title="Pothole Frequency vs Plow Time",
         x = "Plow Time",
         y = "Pothole Frequency")
```


There are many points clustered around the lower left hand side of the graph. This makes it hard to view our data. One way we can make the data less skewed is by taking the log of the axes. 

```{r log_pothole_plow_scatter}
pothole_plow_df %>%
  ggplot(mapping = aes(x=log(plow_visits + 1), y=log(freq_potholes+1))) +
  geom_point() +
  geom_smooth(method=lm) +
  labs(title="Log Pothole Frequency vs Log Plow Time",
         x = "Log Plow Time",
         y = "Log Pothole Frequency")
```


The log transformation made our data much more manageable. There appears to be a weak positive relationship between the log pothole frequency and the log plow time. Let's try to verify this by fitting a linear model. A linear model may not be appropriate for this relationship, but it is a good place to start.

First we need to take preform the logarithmic transformation on our data.

```{r make_log_pp_df}
log_pp_df <- pothole_plow_df %>%
  mutate(log_plow_visits = log2(plow_visits + 1),
         log_freq_potholes = log2(freq_potholes + 1))
  
head(log_pp_df)
```

Now we can create a linear model. broom::glance() will tell us some important statistics such as the $r^2$ value.

```{r linear_analysis}
reg <- lm(log_freq_potholes~log_plow_visits, data=log_pp_df)

reg %>%
  broom::glance()
```

broom::tidy() will tell us the details of the model itself.

```{r lin_anal2}
reg %>%
  broom::tidy()
```

We can see that there is a significant linear relationship between log_plow_visits and log_freq_potholes because the model has a p-value that is near zero. With a parameter estimate of 0.4293803, we know that the relationship is positive.

However, the model has a very low $r^2$ value of only 0.156359. We should [check the assumptions of linear regression](http://sphweb.bumc.bu.edu/otlt/MPH-Modules/BS/R/R5_Correlation-Regression/R5_Correlation-Regression7.html) carefully.

An easy way of testing several assumptions, such as normality and homoscedasticity, is by using plot.

```{r test_assumptions}
plot(reg, which=1:4)
```


The plot of residuals vs fits isn't ideal. It should be symmetrical around residual = 0, but we have many more observations that lie above this line than below. It suggests that our distribution is not linear. The Q-Q plot is also quite bad. Our observations should be around the dotted line, but they are not. These issues suggest that our data doesn't meet the normality assumption. 

Our scale-location plot is not horrible. There is a bit of a pattern, but it isn't too bad. None of the points on Cook's distance plot are close to 1 so that is fine, too.

Since we don't meet the normality assumption and our coefficient of determination is so low, it is not safe to use our linear model. We could try to perform a nonlinear transformation to try to improve our model, but I have not found a good one.



